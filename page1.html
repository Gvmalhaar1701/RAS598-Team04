<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>ForceGauge</title>
  <script type="module">
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
    mermaid.initialize({ startOnLoad: true });
  </script>
  <style>
     main {
      text-align: center;     /* Center-align the text */
      padding: 20px;          /* Add some space inside the main block */
    }

    p {
      text-align: justify;     /* Align text inside paragraph to the left */
      padding-left: 60px;
      padding-right: 60px;   /* Indent the entire paragraph */
      max-width: 800px;
      margin: 0 auto;   
    }

    body {
      font-family: Arial, sans-serif;
      margin: 0;
      background: #f9f9f9;
      color: #333;
    }
    h2 {
    font-size: 1.8em;
    font-style: italic;
  }

    .mermaid {
      max-width: 80%;
      margin: auto;
      background: white;
      border-radius: 10px;
      padding: 20px;
      box-shadow: 0 4px 12px rgba(0,0,0,0.1);
    }

    header {
      background: #FFC627;
      color: black;
      padding: 20px;
      text-align: center;
    }
    
    header h1, p {
      text-align: center;
      margin: 20px auto;
    }

    table {
      margin-left: auto;  /* Auto left margin */
      margin-right: auto; /* Auto right margin */
      border-collapse: seperate;
      border-spacing: 0;
      border: 2px solid #8c1d40;
      border-radius: 15px;
    }

    ul li {
    text-align: justify;
    max-width: 800px;  /* Optional: control line width */
    }

    th, td {
      border: 1px solid #8c1d40;
      padding: 10px;
      text-align: center;
    }
    
    .justified-list-container {
      max-width: 800px;         /* Set the width of the list block */
      margin: 0 auto;           /* Center the block horizontally */
      text-align: justify;      /* Justify the text inside */
    }

    .justified-list-container ul {
      padding-left: 20px;       /* Keeps bullets nicely spaced */
    }

    .justified-list-container li {
      margin-bottom: 10px;
    }  
    nav {
      background: #FFC627;
      padding: 10px;
      text-align: center;
    }

    nav a {
      display: inline-block;
      background-color: #8c1d40;  /* Green background */
      color: white;               /* White text */
      padding: 10px 20px;         /* Space around text */
      border-radius: 20px;        /* Curved corners */
      text-decoration: none;      /* Remove underline */
      font-family: sans-serif;
    }

    nav a:hover {
      background-color: #000;
    }

    main {
      padding: 20px;
      text-align: center;
    }

    select {
      padding: 8px;
      font-size: 1em;
      margin-bottom: 20px;
    }

    .gallery {
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
      gap: 20px;
    }

    .gallery img, .gallery video {
      width: 300px;
      border-radius: 10px;
      box-shadow: 0 4px 10px rgba(0,0,0,0.2);
      cursor: pointer;
      transition: transform 0.2s;
    }

    .gallery img:hover, .gallery video:hover {
      transform: scale(1.05);
    }

    .gallery-section {
      display: none;
    }

    .gallery-section.active {
      display: flex;
    }
    
    /* Modal */
    .modal {
      display: none;
      position: fixed;
      z-index: 10;
      left: 0;
      top: 0;
      width: 100%;
      height: 100%;
      background-color: rgba(0,0,0,0.8);
      align-items: center;
      justify-content: center;
    }

    .modal-content {
      max-width: 90%;
      max-height: 80%;
    }

    .modal-content video, .modal-content img {
      max-width: 90vw;
      max-height: 80vh;
      width: auto;
      height: auto;
      border-radius: 10px;
    }

    .close {
      position: absolute;
      top: 20px;
      right: 40px;
      font-size: 40px;
      color: white;
      cursor: pointer;
    }

    .dropdown-box {
        display: inline-block;
        background-color: #FFC627;
        padding: 15px 20px;
        border-radius: 12px;
        box-shadow: 0 4px 10px rgba(0, 0, 0, 0.15);
        margin-bottom: 20px;
    }

    .tooltip-container {
      position: relative;
      display: inline-block;
      cursor: pointer;
    }

    .tooltip-container img {
      width: 300px;
      border-radius: 10px;
    }

    .tooltip-text {
      visibility: hidden;
      background-color: rgba(0, 0, 0, 0.75);
      color: #fff;
      text-align: center;
      padding: 8px;
      border-radius: 6px;
      position: absolute;
      bottom: 110%;
      left: 50%;
      transform: translateX(-50%);
      opacity: 0;
      transition: opacity 0.3s;
      white-space: nowrap;
    }

    .tooltip-container:hover .tooltip-text {
      visibility: visible;
      opacity: 1;
    }

    a {
      text-decoration: none;
      color: inherit;
    }

    footer {
      background: #FFC627;
      color: black;
      text-align: center;
      padding: 15px;
      margin-top: 40px;
    }


  
  </style>
</head>

<body>

    <header>
      <h1>RAS598: TEAM-04</h1>
      <p>STIFFNESS MEASUREMENT USING FORCE GAUGE</p>
    </header>  
<main>      
      <h1><b>ABOUT</b></h1>
      
      <p>The stiffness of the ankle in the pogo stick-type model is characterized through a separate controlled experiment using a force-displacement test setup. 
      This setup is designed to find out loads to the ankle joint and measure the corresponding load, allowing the finding of accurate force-displacement points.</p>

      <br>
      
      <div class="mermaid">
        graph TD  
          A[Project Setup] --> B[Define Experiment Parameters]  
          B --> C[ROS2-Based Software Configuration]  
          C --> D[Conduct Experiment]  
          D --> E[Data Collection]  
          E --> F[Analysis]  
          F --> G{Iterate?}  
          G -->|Yes| D  
          G -->|No| H[Report Findings - find stiffness]
      </div>
      
      <h1><b>PROJECT SETUP</b></h1>
      
      <h2>Project Setup:</h2>
      <p>The setup contains an object which, for this project, a prototype of a ankle + leg of a quadruped similar to a pogostick model, with a Force Gauge ready to record data</p>
      
      <h2>Define Experiment Parameters:</h2>
      <p>A setup in which the model is fixated to the ground and a string is used to attach the leg and a hook of the force gauge. 
        A set of displacement points are marked and the force gauge readings are taken at those points.
        The values are graphed out and the best fit is made to find the stiffness.</p>
      
      <h2>ROS2-Based Software Configuration:</h2>
      <p>The data of the Force readings are published and subscribed via ROS2 software to visualize the data. A GUI is also constructed using PyQT5 which will:
        1. Visualize the Data (2-D Plot of Force data with respect to time).
        2. Contain a button to start and stop recording
        3. A recording system to save the data into a bagfile or a csv file.</p>
      
      <h2>Conduct Experiment:</h2>
      <p>The prototype is provided with tensile force, with the force gauge recording 
        the force values with respect to time and passed through displacement markers with varying velocities to find out stiffness and damping.</p>
      
      <h2>Data Collection:</h2>
      <p>A best fit line is found with the data obtained is calculated whose slope will give stiffness(force vs displacement) or damping(force vs velocity)</p>
      
      <br>  
      
      <p><em>Experimented data will then be collected to study on the attachment and be iterated with different parameters.</em> </p>
      
      <br>

      <h1><b>HARDWARE CONFIGURATION</b></h1>
        <p>The specimen or test object is fixated on the ground via supports and screws and via strings, the specimen is attached to the hook attachment of a Force Gauge which is placed horizontally and tensile forces are added onto the ankle component to get the reading.
           The Force Gauge will pass through a few marked displacement points and the force readings are plotted to find a best fit.</p>
        

           <div class="dropdown-box">
            <label for="mediaType">Choose media:</label>
            <select id="mediaType" onchange="switchGallery()">
              <option value="images">Images</option>
              <option value="videos">Videos</option>
            </select>
                           
           <div id="images" class="gallery gallery-section active">
              <div class="tooltip-container">
                <img src="force_setup.jpg" alt="Descriptive Image" onclick="openModal(this.src)">
                <div class="tooltip-text">Figure 2: A setup calculation using hand drawn calculations. This does come with a lot of human error.</div>
              </div>
             <img src="ur5_fg.jpg" alt="Image 2" onclick="openModal(this.src)">
             <img src="bestfit.png" alt="Image 3" onclick="openModal(this.src)">
           </div>
         
           <div id="videos" class="gallery gallery-section">
             <video src="video1.mp4" onclick="openModal(this.src)" muted></video>
             <video src="video2.mp4" onclick="openModal(this.src)" muted></video>
             <!-- Add more videos -->
           </div>
         
           <!-- Modal -->
           <div id="popupModal" class="modal" onclick="closeModal()">
             <span class="close">&times;</span>
             <div class="modal-content" id="modalContent"></div>
           </div>
         
           <script>
             function switchGallery() {
               const selection = document.getElementById("mediaType").value;
               document.querySelectorAll('.gallery-section').forEach(section => {
                 section.classList.remove('active');
               });
               document.getElementById(selection).classList.add('active');
             }
         
             function openModal(src) {
               const modal = document.getElementById("popupModal");
               const content = document.getElementById("modalContent");
               content.innerHTML = "";
         
               const ext = src.split('.').pop().toLowerCase();
               if (['mp4', 'webm', 'ogg'].includes(ext)) {
                 const video = document.createElement("video");
                 video.src = src;
                 video.controls = true;
                 video.autoplay = true;
                 content.appendChild(video);
               } else {
                 const img = document.createElement("img");
                 img.src = src;
                 content.appendChild(img);
               }
         
               modal.style.display = "flex";
             }
         
             function closeModal() {
               const modal = document.getElementById("popupModal");
               modal.style.display = "none";
             }
         
             // Optional: Close when clicking the '×'
             document.querySelector(".close").onclick = closeModal;
           </script>
           </div>

      
      
      <p>The project involves using two setups:
  
        1. OptiTrack via ROS2 to drop test objects at predetermined positions. A vision system, will record drop points and trajectories
        2. A force gauge is attached to the test object to measure the stiffness and deformation. The setup will fixate the test object to the ground and force will be measured at different displacements and velocities which forms data to find the best fit for the stiffness and damping of the prototype
        
        The experiment analyzes how factors such as object weight and shape influence it and how it relates to the simulation readings. ROS2 software will use a node to publish the sensors to ensure repeatability in the experiment. The process flow chart of our project is shown in the chart below. </p>
      
      <br>
      
      <h1><b>SENSOR INTEGRATION</b></h1>
      
      <p>This project aims to develop a ROS 2-based robotic manipulation system that integrates OptiTrack motion capture technology , and force gauge for measuring simulation parameters. 
        The goal is to drop them from a random height and angle and utilize the OptiTrack and find the projected trajectories. ROS2 will be used for processing and obtaining sensor information and real-time communication, ensuring seamless coordination between different sensor data. This experiment will help in supporting the simulation data of a similar model in MujoCo.
        The OptiTrack motion capture system will provide for improved tracking. By leveraging ROS 2’s distributed architecture, this project will enhance trajectory adaptation in real-time. The combination of motion capture (OptiTrack) will enable high-accuracy object handling. The findings from this experiment will have broader applications in industrial automation, assistive robotics, and testing.</p>
      
      <br>
      
      <h1><b>INTERFACE AND AUTOMATION</b></h1>
      
      <p>The sensor data will be obtained using ROS2, and custom ROS2 nodes for real-time interaction. The experimentation result and automation will be influenced through:
        
        <div class="justified-list-container"> 
        <ul>
          <li><b>Vision-based Perception:</b> OptiTrack motion capture data will be fused to provide high-accuracy object localization.</li>
          <li><b>Adaptive Motion Planning:</b> ROS2 will handle trajectory updates dynamically, allowing the robot to adjust its movements based on real-time feedback.</li>
          <li><b>Automation:</b>For automation, we used a UR5 to automate the experimental setup to remove the possibiliity of human error and repeatability.Third point</li>
        </ul>
        </div>  
      
        To facilitate monitoring and user interaction, the following interfaces will be developed:
        
        <div class="justified-list-container">
        <ul>
          <li><b>Web-Based Dashboard:</b> A user-friendly interface displaying object tracking data, and force gauge readings.</li>
          <li><b>Data Logging System:</b> All relevant data (object positions, timestamps, drop locations, and trajectory corrections) will be stored in a ROS 2-based database for post-experiment analysis.</li>
          <li><b>UR5 Control:</b> Develop a path planning program for UR5 for movement which facilitates the experiments.</li>
        </ul>
        </div>
      </p>
      
      <br>
      
      <h1><b>CONTROL AND AUTONOMY</b></h1>
      
      <p><b>A low-latency feedback loop</b> will be established to provide real-time feedback to the UR5 controller where sensor data is processed and transmitted.
        This enables the UR5 inverse kinematics and control algorithms to dynamically adjust grip strength and drop execution in real-time.</p>
      <p><b>A High-Level decision-making</b> module will also read long-term trends from sensor feedback to make higher-level decisions.</p>
      
      <br>
  
      <h1><b>RESOURCES NEEDED</b></h1>
      
      <p>Since we heavily require camera sensing and sensor fusion, we would be needing knowledge about Control Systems and Autonomous Algorithms, Computer Vision, Object tracking and Sensor Fusion and Filtering Techniques.
        Furthemore, self study and expert advice will go a long way in covering the gaps.</p>
      <br>
  
      <p><b>Changes in environmental conditions</b>
        
        <div class="justified-list-container">
        <ul>
          <li>To handle variability in the environment, 
            the robot will leverage sensor fusion by combining OptiTrack motion capture for global positioning and force gauge for precise local object tracking, 
            ensuring robust localization and more research data.</li>
          <li>In case of misalignment, 
            the system will implement error recovery strategies, such as reattempting detection and adjusting the pick position.
            Additionally, ROS 2-based dynamic reconfiguration will allow real-time parameter tuning and trajectory modifications, supported by a web-based interface for manual overrides. This integrated approach ensures the system remains resilient and adaptable in dynamic environments, 
            maintaining precise and reliable object manipulation.</li>
        </ul>
        </div>
      
      </p>
  
      <br>
  
      <h1><b>IMPACT</b></h1>
      <p>Our team has no prior experience with ROS2, sensor fusion, or object detection, making this a valuable challenge for us to advance in robotic experimentation. We are focused on developing a robust and standalone test procedure for dropping test materials from a height.
        This process can be applied to material testing, orientation-based drop tests, 
        impact conditioning, and assessing real-world behavior under critical conditions. Such a testing framework could significantly enhance rescue operations by evaluating the impact on a robot when deployed from higher floors, 
        such as the 4th or 5th, ensuring better reliability in high-stakes scenarios.</p>
      
      <br>
      
      <h1><b>ADVISING</b></h1>
      <p>We will be mentored by Dr. Daniel M. Aukes, who has expressed his interest in providing mentoring and access to state-of-the-art hardware facilities. 
        Dr. Aukes' guidance will be a key factor in ensuring the technical aspects of the project are addressed effectively. His demands involve regular progress tracking, strict adherence to project milestones, and active participation in troubleshooting sessions. 
        Other facilities, such as laboratory access and state-of-the-art simulation tools, have been guaranteed to aid in our experimental setup. </p>
      
      <br>
      
      <h1><b>TIMELINE(RECENTLY UPDATED)</b></h1>
      
      <div style="text-align: center;">
        <img src="new_timeline.jpg" alt="Timeline" class="popup-image">
      </div>
      
  
      <br>
  
